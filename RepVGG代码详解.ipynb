{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RepVGG的PaddlePaddle复现\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/652da8d77afb4614be737ef7e630af14f48f766682dd46baa11c208c42b995bf)\n",
    "\n",
    "This is a super simple ConvNet architecture that achieves over 80% top-1 accuracy on ImageNet with a stack of 3x3 conv and ReLU! \n",
    "\n",
    "paper：[https://arxiv.org/abs/2101.03697](http://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 前言\n",
    "Hi Guy！欢迎来到这里，这里是对RepVGG复现以及代码深入讲解，RepVGG通过结构重参数化来使得具有多分支结构的训练模型转化为直筒式单路推理模型，在速度和精度tradeoff下达到了SOTA\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ffea66e7574444e8b44b8999da56c55ec2b686c055294b269f61c7a0fe3e83e5)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c93fe7f8605c4b4388b14942f73ac94bff4258e410d148b38843025a22959126)\n",
    "\n",
    "* 本论文代码基于官方Pytorch代码，更原汁原味\n",
    "* AI Studio有大佬复现过RepVGG，但是没有给出推理模型的转换，本文代码更全，配合代码讲解，更容易理解\n",
    "* 论文解读：[https://github.com/lmk123568/RepVGG_Tutorial/blob/main/RepVGG%E8%AE%BA%E6%96%87.md](http://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## RepVGG基础搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前版本为: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import numpy as np\r\n",
    "import paddle.nn as nn\r\n",
    "\r\n",
    "# 国际惯例，导入所需要的包\r\n",
    "print('当前版本为:',paddle.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\r\n",
    "    result = paddle.nn.Sequential(\r\n",
    "        ('conv',nn.Conv2D(in_channels=in_channels, out_channels=out_channels,kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias_attr=False)),\r\n",
    "        ('bn',nn.BatchNorm2D(num_features=out_channels))\r\n",
    "    )\r\n",
    "    return result\r\n",
    "\r\n",
    "# 构造conv+bn组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建RepVGGBlock模块\r\n",
    "# RepVGG除了最后的池化层和分类层之外，都是清一色RepVGGBlock堆叠，十分简单\r\n",
    "\r\n",
    "class RepVGGBlock(nn.Layer):\r\n",
    "\r\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\r\n",
    "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\r\n",
    "        super(RepVGGBlock, self).__init__()\r\n",
    "        self.deploy = deploy             # deploy是推理部署的意思\r\n",
    "        self.groups = groups             # 输入的特征层分为几组，这是分组卷积概念，单卡GPU不用考虑，默认为1，分组卷积概念详见下面\r\n",
    "        self.in_channels = in_channels   # 输入通道\r\n",
    "\r\n",
    "        assert kernel_size == 3          \r\n",
    "        assert padding == 1              # 为什么这么设置呢，图像padding=1后经过 3x3 卷积之后图像大小不变\r\n",
    "        \r\n",
    "        padding_11 = padding - kernel_size // 2\r\n",
    "        \r\n",
    "        self.nonlinearity = nn.ReLU()\r\n",
    "\r\n",
    "\r\n",
    "        if deploy:\r\n",
    "            self.rbr_reparam = nn.Conv2D(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\r\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias_attr=True, padding_mode=padding_mode)\r\n",
    "        # 定义推理模型时，基本block就是一个简单的 conv2D\r\n",
    "       \r\n",
    "        else:\r\n",
    "            self.rbr_identity = nn.BatchNorm2D(num_features=in_channels) if out_channels == in_channels and stride == 1 else None \r\n",
    "            # 直接连接，类似resnet残差连接，注意当输入通道和输出通道不同时候，只有 1x1 和 3x3 卷积，没有identity，下面网络图自己体会\r\n",
    "\r\n",
    "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups) #3x3卷积+BN\r\n",
    "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)          #1x1卷积+BN\r\n",
    "            print('RepVGG Block, identity = ', self.rbr_identity)   # 这句话就是判断这个block没有identity，没有的话返回None，具体看下图输出\r\n",
    "        # 定义训练模型时，基本block是 identity、1x1 conv_bn、3x3 conv_bn 组合\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        if hasattr(self, 'rbr_reparam'):\r\n",
    "            return self.nonlinearity(self.rbr_reparam(inputs))\r\n",
    "        # 推理阶段, conv2D 后 ReLU\r\n",
    "\r\n",
    "        if self.rbr_identity is None:\r\n",
    "            id_out = 0\r\n",
    "        else:\r\n",
    "            id_out = self.rbr_identity(inputs)\r\n",
    "\r\n",
    "        return self.nonlinearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\r\n",
    "        # 训练阶段，3x3、1x1、identity 相加后 ReLU\r\n",
    "\r\n",
    "\r\n",
    "    def get_equivalent_kernel_bias(self):\r\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)   # 卷积核两个参数 W 和 b 提出来\r\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\r\n",
    "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)  # 为啥可以提出两个参数，看论文公式\r\n",
    "        \r\n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\r\n",
    "    # 先理解 _fuse_bn_tensor 这个是干啥的，这模块就好理解了\r\n",
    "    # 卷积核运算本质就是 W(x)+b，但是为啥 identity 可以提取W、b？看后面\r\n",
    " \r\n",
    "\r\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\r\n",
    "        if kernel1x1 is None:\r\n",
    "            return 0\r\n",
    "        else:\r\n",
    "            return nn.functional.pad(kernel1x1, [1,1,1,1])\r\n",
    "    # 这代码讲的是将 1x1 conv padding 一圈成 3x3 conv，填充的是0\r\n",
    "    #                        [0  0  0] \r\n",
    "    #    [1]  >>>padding>>>  [0  1  0]\r\n",
    "    #                        [0  0  0]   \r\n",
    "\r\n",
    "\r\n",
    "    def _fuse_bn_tensor(self, branch):\r\n",
    "        if branch is None:\r\n",
    "            return 0, 0\r\n",
    "            # 当branch不是3x3、1x1、BN，那就返回 W=0, b=0\r\n",
    "\r\n",
    "        if isinstance(branch, nn.Sequential):\r\n",
    "            kernel = branch.conv.weight          # conv权重\r\n",
    "            running_mean = branch.bn._mean       # BN mean\r\n",
    "            running_var = branch.bn._variance    # BN var\r\n",
    "            gamma = branch.bn.weight             # BN γ \r\n",
    "            beta = branch.bn.bias                # BN β\r\n",
    "            eps = branch.bn._epsilon             # 防止分母为0\r\n",
    "            # 当branch是3x3、1x1时候，返回以上数据，为后面做融合\r\n",
    "\r\n",
    "        else:\r\n",
    "            assert isinstance(branch, nn.BatchNorm2D)\r\n",
    "            if not hasattr(self, 'id_tensor'):\r\n",
    "                input_dim = self.in_channels // self.groups                                       # 通道分组，单个GPU不用考虑，详情去搜索分组卷积\r\n",
    "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)    # 定义新的3x3卷积核，参数为0，这里用到DepthWise，详情去搜索MobileNetV1\r\n",
    "                                                                                                  # 这部分看后面讲解\r\n",
    "                for i in range(self.in_channels):\r\n",
    "                    kernel_value[i, i % input_dim, 1, 1] = 1                                      # 将卷积核对角线部分赋予1\r\n",
    "                self.id_tensor = paddle.to_tensor(kernel_value)\r\n",
    "\r\n",
    "            kernel = self.id_tensor               # conv权重       \r\n",
    "            running_mean = branch._mean           # BN mean\r\n",
    "            running_var = branch._variance        # BN var\r\n",
    "            gamma = branch.weight                 # BN γ\r\n",
    "            beta = branch.bias                    # BN β\r\n",
    "            eps = branch._epsilon                 # 防止分母为0\r\n",
    "            # 当branch是 identity，也即只有BN时候返回以上数据\r\n",
    "\r\n",
    "\r\n",
    "        std = (running_var + eps).sqrt()\r\n",
    "        t = (gamma / std).reshape((-1, 1, 1, 1))\r\n",
    "        # 提取W、b，不管你是 3x3 1x1 identity都要提取\r\n",
    "\r\n",
    "        return kernel * t, beta - running_mean * gamma / std\r\n",
    "        # 细心的读者发现，上述公式没有提到 conv 1x1、conv 3x3 的 bias\r\n",
    "        # 这部分是精华，也是难以理解的部分，希望读者多多阅读代码，推推公式，深入理解原理\r\n",
    "\r\n",
    "\r\n",
    "    def repvgg_convert(self):\r\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\r\n",
    "        return kernel.numpy(), bias.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 代码疑惑部分讲解\n",
    "* Conv怎么和BN融合\n",
    "```\n",
    "std = (running_var + eps).sqrt()\n",
    "t = (gamma / std).reshape((-1, 1, 1, 1))\n",
    "\n",
    "return kernel * t, beta - running_mean * gamma / std\n",
    "```\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/56c7488a4d9e4305a9c4e8535171349227b85b7633b445c6865aebd27107f5db)\n",
    "\n",
    "* Identity怎么转3x3 Conv\n",
    "```\n",
    "if not hasattr(self, 'id_tensor'):\n",
    "    input_dim = self.in_channels // self.groups                                       # 通道分组，单个GPU不用考虑，详情去搜索分组卷积\n",
    "    kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)    # 定义新的3x3卷积核，参数为0，这里用到DepthWise，详情去搜索MobileNetV1\n",
    "                                                                                      # 这部分看后面讲解\n",
    "    for i in range(self.in_channels):\n",
    "        kernel_value[i, i % input_dim, 1, 1] = 1                                      # 将卷积核对角线部分赋予1\n",
    "    self.id_tensor = paddle.to_tensor(kernel_value)\n",
    "```\n",
    "\n",
    "首先我们看分组卷积，图中groups=3，in_channels被分为3份，相应地，Conv channel缩小，降低了一定的参数量，有利于多卡训练，有兴趣请查阅相关文献，本文不做详细讨论\n",
    "  \n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/72247e542a2b438fbaf5d6620ec3822309ddc1df91384685bf32a46048d7f9e8)\n",
    "\n",
    "这次为了好理解代码，设置groups=1，input_dim=in_channel\n",
    "\n",
    "我们来思考一下对于一个Identity，什么样的Conv可以等效\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cf5f66a2682540f0805ad00b67cff8b58350d917cf0b4a2babde0617e714a742)\n",
    "\n",
    "如上图所示，input size为3xHxW，Conv size为3x3x1x1，白色部分是weight=1，灰色部分是weight=0，很容易得出这样的Conv能输出原特征图，达到Identity效果\n",
    "\n",
    "实际上，由论文我们可以知道，1x1 Conv可以用3x3 Conv替代\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b951788bfcd644d38e62ab3661374ee018a48dfb2475422cb5e09d99042af8e6)\n",
    "\n",
    "这样的话，Identity就可以由这样的3x3 Conv替代，3x3Conv size为3x3x3x3，\n",
    "对应代码`kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)`\n",
    "\n",
    "下面看一下这个Conv中权值为1的位置有什么特点\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a446d8af61ae46a09e6fcedd4a748bc3160d0926407844ffb3d7988a182d8576)\n",
    "\n",
    "对应`kernel_value[i, i % input_dim, 1, 1] = 1`，这样的话刚好可以构建上图所示的Conv\n",
    "\n",
    "* RepVGG Block\n",
    "```\n",
    "self.rbr_identity = nn.BatchNorm2D(num_features=in_channels) if out_channels == in_channels and stride == 1 else None \n",
    "\n",
    "self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups) #3x3卷积+BN\n",
    "self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)          #1x1卷积+BN\n",
    "print('RepVGG Block, identity = ', self.rbr_identity)   \n",
    "```\n",
    "\n",
    "* 当RepVGG Block的in_channel ≠ out_channel时，在RepVGG里面负责下采样（stride=2），输入特征图空间维度缩小，特征通道增加，以便提取高层语义特征，此时Block没有Identity，print输出None，a\n",
    "* 当RepVGG Block的in_channel = out_channel时，Block包含三个Branch，stride=1，print输出相应的BN信息，如b\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3f7118f02b6c4236ba6945a463dac7887c17787b233d4e42b3d48828818fe2d8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RepVGG(nn.Layer):\r\n",
    "\r\n",
    "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False):\r\n",
    "        super(RepVGG, self).__init__()\r\n",
    "\r\n",
    "        assert len(width_multiplier) == 4 # 江湖人称瘦身因子，减小网络的宽度，就是输出通道乘以权重变小还是变大\r\n",
    "\r\n",
    "        self.deploy = deploy\r\n",
    "        self.override_groups_map = override_groups_map or dict() # 这部分是分组卷积，单个GPU不用考虑\r\n",
    "\r\n",
    "        assert 0 not in self.override_groups_map\r\n",
    "\r\n",
    "        self.in_planes = min(64, int(64 * width_multiplier[0]))\r\n",
    "\r\n",
    "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy)\r\n",
    "        self.cur_layer_idx = 1                          # 分组卷积\r\n",
    "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\r\n",
    "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\r\n",
    "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\r\n",
    "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\r\n",
    "        self.gap = nn.AdaptiveAvgPool2D(output_size=1)  # 全局池化，变成 Nx1x1（CxHxW），类似 flatten\r\n",
    "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\r\n",
    "        \r\n",
    "\r\n",
    "    def _make_stage(self, planes, num_blocks, stride):\r\n",
    "        strides = [stride] + [1]*(num_blocks-1)\r\n",
    "        blocks = []\r\n",
    "        for stride in strides:\r\n",
    "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1) # 分组卷积\r\n",
    "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\r\n",
    "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy))\r\n",
    "            self.in_planes = planes\r\n",
    "            self.cur_layer_idx += 1\r\n",
    "        return nn.Sequential(*blocks)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        out = self.stage0(x)\r\n",
    "        out = self.stage1(out)\r\n",
    "        out = self.stage2(out)\r\n",
    "        out = self.stage3(out)\r\n",
    "        out = self.stage4(out)\r\n",
    "        out = self.gap(out)\r\n",
    "        out = paddle.flatten(out,start_axis=1)\r\n",
    "        out = self.linear(out)\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## RepVGG模型实例化\n",
    "\n",
    "只要改变上述的参数，可以得出不同规模的网络，作者把网络规模分为A类和B类，配置如下\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c96b1b09871f44dfb5470c09c841b05d8221a8474f4248659908b61bb55286c1)\n",
    "\n",
    "* a，b是缩放系数\n",
    "* 第一个stage处理大分辨率，只设计一个3x3卷积而减小参数量\n",
    "* 最后一层channel很多，只设计一个3x3卷积而减小参数量\n",
    "* 按照ResNet，更多层放到倒数第二个stage\n",
    "* 为了实现下采样，每个stage第一个3x3卷积将stride设置2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\r\n",
    "g2_map = {l: 2 for l in optional_groupwise_layers}\r\n",
    "g4_map = {l: 4 for l in optional_groupwise_layers}\r\n",
    "\r\n",
    "def create_RepVGG_A0(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_A1(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_A2(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)\r\n",
    "\r\n",
    "\r\n",
    "def create_RepVGG_B0(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_B1(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_B1g2(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_B1g4(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[2, 2, 2, 4], override_groups_map=g4_map, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_B2(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_B2g2(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g2_map, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_B2g4(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=g4_map, deploy=deploy)\r\n",
    "\r\n",
    "\r\n",
    "def create_RepVGG_B3(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[3, 3, 3, 5], override_groups_map=None, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_B3g2(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000,\r\n",
    "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g2_map, deploy=deploy)\r\n",
    "\r\n",
    "def create_RepVGG_B3g4(deploy=False,num_classes=10):\r\n",
    "    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=num_classes,\r\n",
    "                  width_multiplier=[3, 3, 3, 5], override_groups_map=g4_map, deploy=deploy)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这次我们选择 RepVGG_A0 作为实例化对象，如果对精度有更高要求，读者也可以选择更大的模型，比如 RepVGG_B2\n",
    "```\n",
    "repvgg_b2=create_RepVGG_B2(num_classes=10)\n",
    "```\n",
    "不同规模网络性能如下\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/230ad30833984dbdbd518e0b95d4c89f0084152a08d04f84809c6bc7cfaca69e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepVGG Block, identity =  None\n",
      "RepVGG Block, identity =  None\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=48, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  None\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=96, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=96, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=96, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  None\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  BatchNorm2D(num_features=192, momentum=0.9, epsilon=1e-05)\n",
      "RepVGG Block, identity =  None\n"
     ]
    }
   ],
   "source": [
    "repvgg_a0=create_RepVGG_A0(num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> repvgg_a0 训练模型可视化，长图警告！！！\n",
    "\n",
    "* 注意这是训练模型，具有多分支结构，resnet已经证明分支结构适合训练提取特征\n",
    "* 如果太长影响阅读，点击右上`^`键，收缩该图\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c7603f5214b4449e9f97a62db1f7d0c952e6f2124a7c44bda84e896e0c24c339)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据准备\n",
    "通过Cifar10简单训练网络，本次训练只用简单的Normalize，没有用过多的数据增强，读者可以自己尝试，也可以用更大的数据集（Cifar100）测试该网络的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /home/aistudio/.cache/paddle/dataset/cifar/cifar-10-python.tar.gz not found, downloading https://dataset.bj.bcebos.com/cifar/cifar-10-python.tar.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n"
     ]
    }
   ],
   "source": [
    "import paddle.vision.transforms as T\n",
    "from paddle.vision.datasets import Cifar10\n",
    "\n",
    "# 数据准备\n",
    "transform = T.Compose([\n",
    "    T.Resize(size=(224,224)),\n",
    "    T.Normalize(mean=[127.5, 127.5, 127.5],std=[127.5, 127.5, 127.5],data_format='HWC'),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = Cifar10(mode='train', transform=transform)\n",
    "val_dataset = Cifar10(mode='test',  transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 高层API训练\n",
    "高层API灵活度不够，暂时没有想到办法通过高层API将 train model 转化成 deploy model，不过大家可以训练一下看看acc如何，作为参考，强烈建议跳过这一部分，看基础API训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "   Layer (type)         Input Shape          Output Shape         Param #    \n",
      "===============================================================================\n",
      "     Conv2D-1        [[1, 3, 224, 224]]   [1, 48, 112, 112]        1,296     \n",
      "   BatchNorm2D-1    [[1, 48, 112, 112]]   [1, 48, 112, 112]         192      \n",
      "     Conv2D-2        [[1, 3, 224, 224]]   [1, 48, 112, 112]         144      \n",
      "   BatchNorm2D-2    [[1, 48, 112, 112]]   [1, 48, 112, 112]         192      \n",
      "      ReLU-1        [[1, 48, 112, 112]]   [1, 48, 112, 112]          0       \n",
      "   RepVGGBlock-1     [[1, 3, 224, 224]]   [1, 48, 112, 112]          0       \n",
      "     Conv2D-3       [[1, 48, 112, 112]]    [1, 48, 56, 56]        20,736     \n",
      "   BatchNorm2D-3     [[1, 48, 56, 56]]     [1, 48, 56, 56]          192      \n",
      "     Conv2D-4       [[1, 48, 112, 112]]    [1, 48, 56, 56]         2,304     \n",
      "   BatchNorm2D-4     [[1, 48, 56, 56]]     [1, 48, 56, 56]          192      \n",
      "      ReLU-2         [[1, 48, 56, 56]]     [1, 48, 56, 56]           0       \n",
      "   RepVGGBlock-2    [[1, 48, 112, 112]]    [1, 48, 56, 56]           0       \n",
      "   BatchNorm2D-5     [[1, 48, 56, 56]]     [1, 48, 56, 56]          192      \n",
      "     Conv2D-5        [[1, 48, 56, 56]]     [1, 48, 56, 56]        20,736     \n",
      "   BatchNorm2D-6     [[1, 48, 56, 56]]     [1, 48, 56, 56]          192      \n",
      "     Conv2D-6        [[1, 48, 56, 56]]     [1, 48, 56, 56]         2,304     \n",
      "   BatchNorm2D-7     [[1, 48, 56, 56]]     [1, 48, 56, 56]          192      \n",
      "      ReLU-3         [[1, 48, 56, 56]]     [1, 48, 56, 56]           0       \n",
      "   RepVGGBlock-3     [[1, 48, 56, 56]]     [1, 48, 56, 56]           0       \n",
      "     Conv2D-7        [[1, 48, 56, 56]]     [1, 96, 28, 28]        41,472     \n",
      "   BatchNorm2D-8     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "     Conv2D-8        [[1, 48, 56, 56]]     [1, 96, 28, 28]         4,608     \n",
      "   BatchNorm2D-9     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "      ReLU-4         [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "   RepVGGBlock-4     [[1, 48, 56, 56]]     [1, 96, 28, 28]           0       \n",
      "  BatchNorm2D-10     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "     Conv2D-9        [[1, 96, 28, 28]]     [1, 96, 28, 28]        82,944     \n",
      "  BatchNorm2D-11     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "     Conv2D-10       [[1, 96, 28, 28]]     [1, 96, 28, 28]         9,216     \n",
      "  BatchNorm2D-12     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "      ReLU-5         [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "   RepVGGBlock-5     [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "  BatchNorm2D-13     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "     Conv2D-11       [[1, 96, 28, 28]]     [1, 96, 28, 28]        82,944     \n",
      "  BatchNorm2D-14     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "     Conv2D-12       [[1, 96, 28, 28]]     [1, 96, 28, 28]         9,216     \n",
      "  BatchNorm2D-15     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "      ReLU-6         [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "   RepVGGBlock-6     [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "  BatchNorm2D-16     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "     Conv2D-13       [[1, 96, 28, 28]]     [1, 96, 28, 28]        82,944     \n",
      "  BatchNorm2D-17     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "     Conv2D-14       [[1, 96, 28, 28]]     [1, 96, 28, 28]         9,216     \n",
      "  BatchNorm2D-18     [[1, 96, 28, 28]]     [1, 96, 28, 28]          384      \n",
      "      ReLU-7         [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "   RepVGGBlock-7     [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "     Conv2D-15       [[1, 96, 28, 28]]     [1, 192, 14, 14]       165,888    \n",
      "  BatchNorm2D-19     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-16       [[1, 96, 28, 28]]     [1, 192, 14, 14]       18,432     \n",
      "  BatchNorm2D-20     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-8         [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "   RepVGGBlock-8     [[1, 96, 28, 28]]     [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-21     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-17       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-22     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-18       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-23     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-9         [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "   RepVGGBlock-9     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-24     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-19       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-25     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-20       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-26     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-10        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-10     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-27     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-21       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-28     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-22       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-29     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-11        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-11     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-30     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-23       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-31     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-24       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-32     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-12        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-12     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-33     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-25       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-34     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-26       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-35     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-13        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-13     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-36     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-27       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-37     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-28       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-38     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-14        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-14     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-39     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-29       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-40     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-30       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-41     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-15        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-15     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-42     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-31       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-43     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-32       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-44     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-16        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-16     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-45     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-33       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-46     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-34       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-47     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-17        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-17     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-48     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-35       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-49     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-36       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-50     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-18        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-18     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-51     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-37       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-52     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-38       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-53     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-19        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-19     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-54     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-39       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-55     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-40       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-56     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-20        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-20     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  BatchNorm2D-57     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-41       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,776    \n",
      "  BatchNorm2D-58     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "     Conv2D-42       [[1, 192, 14, 14]]    [1, 192, 14, 14]       36,864     \n",
      "  BatchNorm2D-59     [[1, 192, 14, 14]]    [1, 192, 14, 14]         768      \n",
      "      ReLU-21        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-21     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-43       [[1, 192, 14, 14]]    [1, 1280, 7, 7]       2,211,840   \n",
      "  BatchNorm2D-60     [[1, 1280, 7, 7]]     [1, 1280, 7, 7]         5,120     \n",
      "     Conv2D-44       [[1, 192, 14, 14]]    [1, 1280, 7, 7]        245,760    \n",
      "  BatchNorm2D-61     [[1, 1280, 7, 7]]     [1, 1280, 7, 7]         5,120     \n",
      "      ReLU-22        [[1, 1280, 7, 7]]     [1, 1280, 7, 7]           0       \n",
      "  RepVGGBlock-22     [[1, 192, 14, 14]]    [1, 1280, 7, 7]           0       \n",
      "AdaptiveAvgPool2D-1  [[1, 1280, 7, 7]]     [1, 1280, 1, 1]           0       \n",
      "     Linear-1           [[1, 1280]]            [1, 10]            12,810     \n",
      "===============================================================================\n",
      "Total params: 7,864,426\n",
      "Trainable params: 7,817,130\n",
      "Non-trainable params: 47,296\n",
      "-------------------------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 88.73\n",
      "Params size (MB): 30.00\n",
      "Estimated Total Size (MB): 119.30\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 7864426, 'trainable_params': 7817130}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 高层API\n",
    "model = paddle.Model(repvgg_a0)\n",
    "model.summary((1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开始训练，也可以不训练，不影响后面运行，建议跳过这一部分\r\n",
    "model.prepare(optimizer=paddle.optimizer.Adam(learning_rate=0.001,parameters=model.parameters()),\r\n",
    "              loss=paddle.nn.CrossEntropyLoss(),\r\n",
    "              metrics=paddle.metric.Accuracy())\r\n",
    "\r\n",
    "vdl_callback = paddle.callbacks.VisualDL(log_dir='log') # 训练可视化\r\n",
    "\r\n",
    "model.fit(\r\n",
    "    train_data=train_dataset, \r\n",
    "    eval_data=val_dataset, \r\n",
    "    batch_size=64, \r\n",
    "    epochs=10, \r\n",
    "    save_dir='save_models', \r\n",
    "    verbose=1, \r\n",
    "    callbacks=vdl_callback # 训练可视化\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 基础API训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代器第一轮批次：\n",
      "图片数据 [128, 3, 224, 224]\n",
      "标签数据 [128]\n",
      "\n",
      "标签数据需要利用paddle.unsqueeze()变成[128,1]\n"
     ]
    }
   ],
   "source": [
    "train_batch = paddle.io.DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "val_batch = paddle.io.DataLoader(val_dataset, batch_size=128, shuffle=True , drop_last=True)\n",
    "\n",
    "for i in train_batch:\n",
    "    print('迭代器第一轮批次：')\n",
    "    print('图片数据',i[0].shape)\n",
    "    print('标签数据',i[1].shape)\n",
    "    break\n",
    "\n",
    "print('')\n",
    "print('标签数据需要利用paddle.unsqueeze()变成[128,1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:648: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: [2.377829], acc is: [0.1015625]\n",
      "epoch: 0, batch_id: 100, loss is: [1.6956916], acc is: [0.34375]\n",
      "epoch: 0, batch_id: 200, loss is: [1.2747836], acc is: [0.515625]\n",
      "epoch: 0, batch_id: 300, loss is: [1.0669378], acc is: [0.609375]\n",
      "batch_id: 0, loss is: [1.1263782], acc is: [0.609375]\n",
      "epoch: 1, batch_id: 0, loss is: [1.0189099], acc is: [0.5859375]\n",
      "epoch: 1, batch_id: 100, loss is: [0.98348755], acc is: [0.6640625]\n",
      "epoch: 1, batch_id: 200, loss is: [0.8737333], acc is: [0.6640625]\n",
      "epoch: 1, batch_id: 300, loss is: [0.9537698], acc is: [0.6796875]\n",
      "batch_id: 0, loss is: [0.6566773], acc is: [0.75]\n",
      "epoch: 2, batch_id: 0, loss is: [0.65497804], acc is: [0.8046875]\n",
      "epoch: 2, batch_id: 100, loss is: [0.501753], acc is: [0.8359375]\n",
      "epoch: 2, batch_id: 200, loss is: [0.6873904], acc is: [0.765625]\n",
      "epoch: 2, batch_id: 300, loss is: [0.7049354], acc is: [0.765625]\n",
      "batch_id: 0, loss is: [0.7807969], acc is: [0.734375]\n",
      "epoch: 3, batch_id: 0, loss is: [0.54755455], acc is: [0.828125]\n",
      "epoch: 3, batch_id: 100, loss is: [0.50581074], acc is: [0.859375]\n",
      "epoch: 3, batch_id: 200, loss is: [0.6492828], acc is: [0.7734375]\n",
      "epoch: 3, batch_id: 300, loss is: [0.4320676], acc is: [0.8515625]\n",
      "batch_id: 0, loss is: [0.7076975], acc is: [0.7421875]\n",
      "epoch: 4, batch_id: 0, loss is: [0.3518965], acc is: [0.875]\n",
      "epoch: 4, batch_id: 100, loss is: [0.37317213], acc is: [0.875]\n",
      "epoch: 4, batch_id: 200, loss is: [0.52790904], acc is: [0.8359375]\n",
      "epoch: 4, batch_id: 300, loss is: [0.35826653], acc is: [0.84375]\n",
      "batch_id: 0, loss is: [0.6337395], acc is: [0.7734375]\n",
      "epoch: 5, batch_id: 0, loss is: [0.38129145], acc is: [0.84375]\n",
      "epoch: 5, batch_id: 100, loss is: [0.25682098], acc is: [0.8984375]\n",
      "epoch: 5, batch_id: 200, loss is: [0.38154456], acc is: [0.8359375]\n",
      "epoch: 5, batch_id: 300, loss is: [0.25734812], acc is: [0.890625]\n",
      "batch_id: 0, loss is: [0.45733023], acc is: [0.8515625]\n",
      "epoch: 6, batch_id: 0, loss is: [0.2598531], acc is: [0.8828125]\n",
      "epoch: 6, batch_id: 100, loss is: [0.26805323], acc is: [0.90625]\n",
      "epoch: 6, batch_id: 200, loss is: [0.32644916], acc is: [0.875]\n",
      "epoch: 6, batch_id: 300, loss is: [0.28311527], acc is: [0.921875]\n",
      "batch_id: 0, loss is: [0.61987674], acc is: [0.828125]\n",
      "epoch: 7, batch_id: 0, loss is: [0.2908793], acc is: [0.90625]\n",
      "epoch: 7, batch_id: 100, loss is: [0.28610078], acc is: [0.90625]\n",
      "epoch: 7, batch_id: 200, loss is: [0.14960657], acc is: [0.953125]\n",
      "epoch: 7, batch_id: 300, loss is: [0.08674078], acc is: [0.96875]\n",
      "batch_id: 0, loss is: [0.69591975], acc is: [0.796875]\n",
      "epoch: 8, batch_id: 0, loss is: [0.14409035], acc is: [0.953125]\n",
      "epoch: 8, batch_id: 100, loss is: [0.1729168], acc is: [0.9375]\n",
      "epoch: 8, batch_id: 200, loss is: [0.24014926], acc is: [0.90625]\n",
      "epoch: 8, batch_id: 300, loss is: [0.10751949], acc is: [0.96875]\n",
      "batch_id: 0, loss is: [0.5920343], acc is: [0.8203125]\n",
      "epoch: 9, batch_id: 0, loss is: [0.06296505], acc is: [0.9921875]\n",
      "epoch: 9, batch_id: 100, loss is: [0.08444205], acc is: [0.9609375]\n",
      "epoch: 9, batch_id: 200, loss is: [0.18563382], acc is: [0.9296875]\n",
      "epoch: 9, batch_id: 300, loss is: [0.15956914], acc is: [0.9609375]\n",
      "batch_id: 0, loss is: [0.33377635], acc is: [0.875]\n"
     ]
    }
   ],
   "source": [
    "def fit(model,train_batch,val_batch,epoch):\n",
    "\n",
    "    # 参数optimizer设置优化器，参数loss损失函数\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.001,parameters=model.parameters())\n",
    "    # 参数loss损失函数\n",
    "    loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    for epoch_id in range(epoch):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_id,batch_data in enumerate(train_batch):\n",
    "\n",
    "            input_batch = batch_data[0]\n",
    "            label_batch = paddle.unsqueeze(batch_data[1],axis=1)  # 标签维度变化\n",
    "\n",
    "            predict = model(input_batch)\n",
    "\n",
    "            loss = loss_fn(predict, label_batch)\n",
    "            acc = paddle.metric.accuracy(predict, label_batch)\n",
    "\n",
    "            # 反向传播\n",
    "            loss.backward()   \n",
    "            # 更新参数\n",
    "            opt.step()\n",
    "            # 梯度清零\n",
    "            opt.clear_grad()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch_id, batch_id, loss.numpy(), acc.numpy()))\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        for batch_id,batch_data in enumerate(val_batch):\n",
    "\n",
    "            img_batch = batch_data[0]\n",
    "            label_batch = paddle.unsqueeze(batch_data[1],axis=1)\n",
    "\n",
    "            predict = model(img_batch)\n",
    "\n",
    "            loss = loss_fn(predict, label_batch)\n",
    "            acc = paddle.metric.accuracy(predict, label_batch)\n",
    "\n",
    "            if batch_id % 20 == 0:\n",
    "                print(\"batch_id: {}, loss is: {}, acc is: {}\".format(batch_id, loss.numpy(), acc.numpy()))\n",
    "\n",
    "fit(repvgg_a0, train_batch, val_batch, epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练模型转换推理模型\n",
    "\n",
    "我们知道，多分支结构适合训练，去拟合数据，防止梯度爆炸梯度消失，但是多分支结构有很多不足，比如占内存，每一个分支就要占据一部分内存，比如推理速度变慢，mac成本高，而对于传统VGG来说，虽然精度不是很高，但是其推理速度十分优秀，原因是直筒式结构，单路一路到底的卷积，尤其是 3x3 卷积，得益于现有计算库比如CuDNN，其计算密度比其他卷积高不少，所以基于此作者提出了结构重参数化，将1x1，identity分支参数融合到3x3，使得RepVGG模型在推理阶段是一路的3x3卷积到底，在速度与精度部分实现了SOTA\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b4b294850d3749858726cbfba7ca64a381dd03ea7ce34e4a83e6fcd8f53b970f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型转换\n",
    "def repvgg_model_convert(model, build_func):\n",
    "    converted_weights = {}  # 将训练模型各层 W 和 bias 存入字典\n",
    "    for name, module in model.named_sublayers():\n",
    "        if hasattr(module, 'repvgg_convert'):\n",
    "            kernel, bias = module.repvgg_convert()\n",
    "            converted_weights[name + '.rbr_reparam.weight'] = kernel\n",
    "            converted_weights[name + '.rbr_reparam.bias'] = bias\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            converted_weights[name + '.weight'] = module.weight.numpy()\n",
    "            converted_weights[name + '.bias'] = module.bias.numpy()\n",
    "\n",
    "    deploy_model = build_func\n",
    "    for name, param in deploy_model.named_parameters():\n",
    "        print('deploy param: ', name, np.mean(converted_weights[name]))\n",
    "        param.data = paddle.to_tensor(converted_weights[name])\n",
    "\n",
    "    return deploy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploy param:  stage0.rbr_reparam.weight -0.012085067\n",
      "deploy param:  stage0.rbr_reparam.bias 0.045714345\n",
      "deploy param:  stage1.0.rbr_reparam.weight -4.6854173e-05\n",
      "deploy param:  stage1.0.rbr_reparam.bias -0.020605326\n",
      "deploy param:  stage1.1.rbr_reparam.weight 0.0008855257\n",
      "deploy param:  stage1.1.rbr_reparam.bias -0.12333813\n",
      "deploy param:  stage2.0.rbr_reparam.weight -0.00063567486\n",
      "deploy param:  stage2.0.rbr_reparam.bias 0.23614872\n",
      "deploy param:  stage2.1.rbr_reparam.weight -0.00056437775\n",
      "deploy param:  stage2.1.rbr_reparam.bias 0.24321677\n",
      "deploy param:  stage2.2.rbr_reparam.weight -0.00067081465\n",
      "deploy param:  stage2.2.rbr_reparam.bias 0.2832947\n",
      "deploy param:  stage2.3.rbr_reparam.weight -0.0004131663\n",
      "deploy param:  stage2.3.rbr_reparam.bias 0.04254788\n",
      "deploy param:  stage3.0.rbr_reparam.weight -0.0011545079\n",
      "deploy param:  stage3.0.rbr_reparam.bias 0.5689105\n",
      "deploy param:  stage3.1.rbr_reparam.weight -0.00042346717\n",
      "deploy param:  stage3.1.rbr_reparam.bias 0.020992994\n",
      "deploy param:  stage3.2.rbr_reparam.weight -0.00041332928\n",
      "deploy param:  stage3.2.rbr_reparam.bias 0.03356257\n",
      "deploy param:  stage3.3.rbr_reparam.weight -0.00028026314\n",
      "deploy param:  stage3.3.rbr_reparam.bias -0.103242874\n",
      "deploy param:  stage3.4.rbr_reparam.weight -0.00030400493\n",
      "deploy param:  stage3.4.rbr_reparam.bias -0.0461667\n",
      "deploy param:  stage3.5.rbr_reparam.weight -4.5831683e-05\n",
      "deploy param:  stage3.5.rbr_reparam.bias -0.26605505\n",
      "deploy param:  stage3.6.rbr_reparam.weight 6.658539e-05\n",
      "deploy param:  stage3.6.rbr_reparam.bias -0.32315442\n",
      "deploy param:  stage3.7.rbr_reparam.weight 9.768512e-05\n",
      "deploy param:  stage3.7.rbr_reparam.bias -0.32622793\n",
      "deploy param:  stage3.8.rbr_reparam.weight 0.000116009425\n",
      "deploy param:  stage3.8.rbr_reparam.bias -0.2892122\n",
      "deploy param:  stage3.9.rbr_reparam.weight 0.00022997792\n",
      "deploy param:  stage3.9.rbr_reparam.bias -0.36509892\n",
      "deploy param:  stage3.10.rbr_reparam.weight 0.00022792583\n",
      "deploy param:  stage3.10.rbr_reparam.bias -0.35063317\n",
      "deploy param:  stage3.11.rbr_reparam.weight 0.00033351785\n",
      "deploy param:  stage3.11.rbr_reparam.bias -0.3873196\n",
      "deploy param:  stage3.12.rbr_reparam.weight 0.0002637774\n",
      "deploy param:  stage3.12.rbr_reparam.bias -0.32720482\n",
      "deploy param:  stage3.13.rbr_reparam.weight 4.850667e-05\n",
      "deploy param:  stage3.13.rbr_reparam.bias -0.097426474\n",
      "deploy param:  stage4.0.rbr_reparam.weight 7.6041026e-05\n",
      "deploy param:  stage4.0.rbr_reparam.bias -0.17974474\n",
      "deploy param:  linear.weight -0.0033639586\n",
      "deploy param:  linear.bias 2.7480442e-05\n"
     ]
    }
   ],
   "source": [
    "deploy_model = repvgg_model_convert(repvgg_a0, create_RepVGG_A0(deploy=True,num_classes=10))\n",
    "# 输出每一block参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id: 0, loss is: [0.66172945], acc is: [0.765625]\n",
      "batch_id: 20, loss is: [1.0397379], acc is: [0.7890625]\n",
      "batch_id: 40, loss is: [0.89671266], acc is: [0.7890625]\n",
      "batch_id: 60, loss is: [0.49206328], acc is: [0.84375]\n"
     ]
    }
   ],
   "source": [
    "# 得到的deploy_model在Cifar10数据集进行eval\n",
    "deploy_model.eval()\n",
    "\n",
    "loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "\n",
    "for batch_id,batch_data in enumerate(val_batch):\n",
    "\n",
    "    img_batch = batch_data[0]\n",
    "    label_batch = paddle.unsqueeze(batch_data[1],axis=1)\n",
    "\n",
    "    predict = repvgg_a0(img_batch)\n",
    "\n",
    "    loss = loss_fn(predict, label_batch)\n",
    "    acc = paddle.metric.accuracy(predict, label_batch)\n",
    "\n",
    "    if batch_id % 20 == 0:\n",
    "        print(\"batch_id: {}, loss is: {}, acc is: {}\".format(batch_id, loss.numpy(), acc.numpy()))\n",
    "\n",
    "# 和上面训练模型比一下 acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "   Layer (type)         Input Shape          Output Shape         Param #    \n",
      "===============================================================================\n",
      "     Conv2D-45       [[1, 3, 224, 224]]   [1, 48, 112, 112]        1,344     \n",
      "      ReLU-23       [[1, 48, 112, 112]]   [1, 48, 112, 112]          0       \n",
      "  RepVGGBlock-23     [[1, 3, 224, 224]]   [1, 48, 112, 112]          0       \n",
      "     Conv2D-46      [[1, 48, 112, 112]]    [1, 48, 56, 56]        20,784     \n",
      "      ReLU-24        [[1, 48, 56, 56]]     [1, 48, 56, 56]           0       \n",
      "  RepVGGBlock-24    [[1, 48, 112, 112]]    [1, 48, 56, 56]           0       \n",
      "     Conv2D-47       [[1, 48, 56, 56]]     [1, 48, 56, 56]        20,784     \n",
      "      ReLU-25        [[1, 48, 56, 56]]     [1, 48, 56, 56]           0       \n",
      "  RepVGGBlock-25     [[1, 48, 56, 56]]     [1, 48, 56, 56]           0       \n",
      "     Conv2D-48       [[1, 48, 56, 56]]     [1, 96, 28, 28]        41,568     \n",
      "      ReLU-26        [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "  RepVGGBlock-26     [[1, 48, 56, 56]]     [1, 96, 28, 28]           0       \n",
      "     Conv2D-49       [[1, 96, 28, 28]]     [1, 96, 28, 28]        83,040     \n",
      "      ReLU-27        [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "  RepVGGBlock-27     [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "     Conv2D-50       [[1, 96, 28, 28]]     [1, 96, 28, 28]        83,040     \n",
      "      ReLU-28        [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "  RepVGGBlock-28     [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "     Conv2D-51       [[1, 96, 28, 28]]     [1, 96, 28, 28]        83,040     \n",
      "      ReLU-29        [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "  RepVGGBlock-29     [[1, 96, 28, 28]]     [1, 96, 28, 28]           0       \n",
      "     Conv2D-52       [[1, 96, 28, 28]]     [1, 192, 14, 14]       166,080    \n",
      "      ReLU-30        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-30     [[1, 96, 28, 28]]     [1, 192, 14, 14]          0       \n",
      "     Conv2D-53       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-31        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-31     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-54       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-32        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-32     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-55       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-33        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-33     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-56       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-34        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-34     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-57       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-35        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-35     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-58       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-36        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-36     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-59       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-37        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-37     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-60       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-38        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-38     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-61       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-39        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-39     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-62       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-40        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-40     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-63       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-41        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-41     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-64       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-42        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-42     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-65       [[1, 192, 14, 14]]    [1, 192, 14, 14]       331,968    \n",
      "      ReLU-43        [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "  RepVGGBlock-43     [[1, 192, 14, 14]]    [1, 192, 14, 14]          0       \n",
      "     Conv2D-66       [[1, 192, 14, 14]]    [1, 1280, 7, 7]       2,213,120   \n",
      "      ReLU-44        [[1, 1280, 7, 7]]     [1, 1280, 7, 7]           0       \n",
      "  RepVGGBlock-44     [[1, 192, 14, 14]]    [1, 1280, 7, 7]           0       \n",
      "AdaptiveAvgPool2D-2  [[1, 1280, 7, 7]]     [1, 1280, 1, 1]           0       \n",
      "     Linear-2           [[1, 1280]]            [1, 10]            12,810     \n",
      "===============================================================================\n",
      "Total params: 7,041,194\n",
      "Trainable params: 7,041,194\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 41.07\n",
      "Params size (MB): 26.86\n",
      "Estimated Total Size (MB): 68.50\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 7041194, 'trainable_params': 7041194}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型\n",
    "# print(deploy_model)\n",
    "\n",
    "# 高阶封装查看\n",
    "deploy_model_hapi=paddle.Model(deploy_model)\n",
    "deploy_model_hapi.summary((1,3,224,224))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "和上面训练模型（高阶封装）对比一下大小，这个是不是很小呀，才68m左右，一路的3x3卷积是不是特别容易部署\n",
    "\n",
    "有人问，为什么要特意强调3x3卷积，别的卷积不行吗\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/31ccb2be06434018b146705b1b0b7b46f2f7876d8dac42b78f394088a32b5da9)\n",
    "\n",
    "现有的计算库比如CUDA对3x3运算支持有很大的优势，上图可以看出其计算密度（FLOPs/推理时间）达到了38.10!\n",
    "\n",
    "尤其是对芯片比如FPGA实现，只需要定义简单的算子，就能实现一流的结果\n",
    "\n",
    "作者在结尾补充，在低端cpu设备，mobilenetv3还是有优势，但是在低端gpu设备下，repvgg优势还是很明显\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> repvgg_a0 推理模型可视化\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fbc60174ae374f8c9967b897050a00ca072d870a1bd44c629a9ecaa0b24d058a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Summary\n",
    "恭喜你一路下滑看到这里，相信你有所收获\n",
    "\n",
    "总结就不写了，手酸233，下次补上，主要内容都在Github里面了，就是上面的链接\n",
    "\n",
    "本文长期保持维护更新！\n",
    "\n",
    "还有自我介绍，就不写了，大家关注我就好啦\n",
    "\n",
    "长期活跃在AI studio，研究方向是OD（目标检测），每天都在线，有疑问欢迎大家提问，尽力详细回复\n",
    "\n",
    "Github主页：[https://github.com/lmk123568](http://)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
